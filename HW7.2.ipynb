{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feedback.csv', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    lines = [line[:-1] for line in lines]\n",
    "    cols = lines[0].split(',')\n",
    "    lines = lines[1:]\n",
    "    lines = [line.split(',') for line in lines]\n",
    "    lines = [[*line[:4], \" \".join(line[4:])] for line in lines]\n",
    "\n",
    "dataset = pd.DataFrame(lines, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>product_id</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8342</td>\n",
       "      <td>Александр</td>\n",
       "      <td>2017-04-12</td>\n",
       "      <td>\"6 входов  предохранитель\" \"искрит при включен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>8342</td>\n",
       "      <td>Елена</td>\n",
       "      <td>2015-08-04</td>\n",
       "      <td>\"Я являюсь пользователем Пилотов уже больше 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5311</td>\n",
       "      <td>Леонид</td>\n",
       "      <td>2017-07-16</td>\n",
       "      <td>\"хорошо мелет\" \"маркий  остаются следы\" \"Испол...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5311</td>\n",
       "      <td>Сергей</td>\n",
       "      <td>2017-06-28</td>\n",
       "      <td>Компактная \"Не нашел\" \"Работает на ура. В инст...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5311</td>\n",
       "      <td>Ольга</td>\n",
       "      <td>2017-01-21</td>\n",
       "      <td>\"Цена и качество\" Нет \"В использовании 2 месяц...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rating product_id       name        date  \\\n",
       "0      1       8342  Александр  2017-04-12   \n",
       "1      5       8342      Елена  2015-08-04   \n",
       "2      5       5311     Леонид  2017-07-16   \n",
       "3      4       5311     Сергей  2017-06-28   \n",
       "4      5       5311      Ольга  2017-01-21   \n",
       "\n",
       "                                            feedback  \n",
       "0  \"6 входов  предохранитель\" \"искрит при включен...  \n",
       "1  \"Я являюсь пользователем Пилотов уже больше 10...  \n",
       "2  \"хорошо мелет\" \"маркий  остаются следы\" \"Испол...  \n",
       "3  Компактная \"Не нашел\" \"Работает на ура. В инст...  \n",
       "4  \"Цена и качество\" Нет \"В использовании 2 месяц...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feedback(text: str):\n",
    "    text = text.replace(',', ' ')\n",
    "    return ''.join([c if c.isdigit() or c.isalpha() else ' ' for c in text.lower()])\n",
    "\n",
    "dataset['feedback'] = dataset['feedback'].apply(process_feedback)\n",
    "dataset['rating'] = dataset['rating'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>product_id</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8342</td>\n",
       "      <td>Александр</td>\n",
       "      <td>2017-04-12</td>\n",
       "      <td>\"6 входов  предохранитель\" \"искрит при включен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>8342</td>\n",
       "      <td>Елена</td>\n",
       "      <td>2015-08-04</td>\n",
       "      <td>\"Я являюсь пользователем Пилотов уже больше 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5311</td>\n",
       "      <td>Леонид</td>\n",
       "      <td>2017-07-16</td>\n",
       "      <td>\"хорошо мелет\" \"маркий  остаются следы\" \"Испол...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5311</td>\n",
       "      <td>Сергей</td>\n",
       "      <td>2017-06-28</td>\n",
       "      <td>Компактная \"Не нашел\" \"Работает на ура. В инст...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5311</td>\n",
       "      <td>Ольга</td>\n",
       "      <td>2017-01-21</td>\n",
       "      <td>\"Цена и качество\" Нет \"В использовании 2 месяц...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rating product_id       name        date  \\\n",
       "0      1       8342  Александр  2017-04-12   \n",
       "1      5       8342      Елена  2015-08-04   \n",
       "2      5       5311     Леонид  2017-07-16   \n",
       "3      4       5311     Сергей  2017-06-28   \n",
       "4      5       5311      Ольга  2017-01-21   \n",
       "\n",
       "                                            feedback  \n",
       "0  \"6 входов  предохранитель\" \"искрит при включен...  \n",
       "1  \"Я являюсь пользователем Пилотов уже больше 10...  \n",
       "2  \"хорошо мелет\" \"маркий  остаются следы\" \"Испол...  \n",
       "3  Компактная \"Не нашел\" \"Работает на ура. В инст...  \n",
       "4  \"Цена и качество\" Нет \"В использовании 2 месяц...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset['feedback'], dataset['rating'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming + word encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "\n",
    "def stem(data):\n",
    "    stemmer = SnowballStemmer('russian')\n",
    "    return data.apply(lambda text: [stemmer.stem(word) for word in text.split(' ')])\n",
    "\n",
    "def clear_words(data):\n",
    "    return data.apply(lambda l: [word for word in l if word.isalpha()])\n",
    "\n",
    "def encode_words(train, test):\n",
    "    train = clear_words(train)\n",
    "    test = clear_words(test)\n",
    "    \n",
    "    words = reduce(lambda x, y: set(x).union(set(y)), chain(train, test))\n",
    "    words_map = {word: i + 2 for i, word in enumerate(words)}\n",
    "    print(len(words) + 2) \n",
    "    \n",
    "    mapping_func = lambda l: [words_map[word] for word in l] + [0]\n",
    "    return train.apply(mapping_func), test.apply(mapping_func)\n",
    "\n",
    "def equalize(train, test, length):\n",
    "    eq_func = lambda l: l[:length] if len(l) >= length else l + [1] * (length - len(l))\n",
    "    return train.apply(eq_func), test.apply(eq_func)\n",
    "\n",
    "def clear_zero_lengths(data):\n",
    "    return data.where(data.apply(len) != 0)\n",
    "\n",
    "def to_array(series):\n",
    "    return np.array(list(series.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stem = stem(X_train)\n",
    "X_test_stem = stem(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d179f3c965bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_stem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_stem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-156313c96adb>\u001b[0m in \u001b[0;36mencode_words\u001b[0;34m(train, test)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclear_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mwords_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-156313c96adb>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclear_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mwords_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_encoded, X_test_encoded = encode_words(X_train_stem, X_test_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded.apply(len).hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37.0, 36.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded.apply(len).median(), X_test_encoded.apply(len).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tr, X_test_tr = equalize(X_train_encoded, X_test_encoded, length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = to_array(X_train_tr)\n",
    "X_test_final = to_array(X_test_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM (continious target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11555 samples, validate on 1284 samples\n",
      "Epoch 1/4\n",
      "11555/11555 [==============================] - 86s 7ms/sample - loss: 2.0031 - mean_squared_error: 2.0031 - val_loss: 1.0864 - val_mean_squared_error: 1.0864\n",
      "Epoch 2/4\n",
      "11555/11555 [==============================] - 84s 7ms/sample - loss: 0.8968 - mean_squared_error: 0.8968 - val_loss: 0.8681 - val_mean_squared_error: 0.8681\n",
      "Epoch 3/4\n",
      "11555/11555 [==============================] - 83s 7ms/sample - loss: 0.5702 - mean_squared_error: 0.5702 - val_loss: 0.8785 - val_mean_squared_error: 0.8785\n",
      "Epoch 4/4\n",
      "11555/11555 [==============================] - 82s 7ms/sample - loss: 0.4433 - mean_squared_error: 0.4433 - val_loss: 0.8592 - val_mean_squared_error: 0.8592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f761f153c50>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Embedding(25816, 128))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.005), loss=tf.keras.losses.mean_squared_error, metrics=[tf.keras.metrics.mean_squared_error])\n",
    "model.fit(X_train_final, y_train, batch_size=128, epochs=4, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9129527709337447"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test_final)\n",
    "mean_squared_error(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.26      0.35       521\n",
      "           2       0.20      0.33      0.25       314\n",
      "           3       0.20      0.23      0.22       447\n",
      "           4       0.26      0.34      0.29       924\n",
      "           5       0.83      0.77      0.80      3297\n",
      "\n",
      "   micro avg       0.58      0.58      0.58      5503\n",
      "   macro avg       0.41      0.39      0.38      5503\n",
      "weighted avg       0.62      0.58      0.59      5503\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5785934944575686"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(y_test.round().astype('int'), predictions.round().astype('int')))\n",
    "accuracy_score(y_test.round().astype('int'), predictions.round().astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM (multiclass target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "y_train_ohe = OneHotEncoder().fit_transform(y_train.astype('float').round().astype('int').reshape(-1, 1)).todense()\n",
    "y_test_ohe = OneHotEncoder().fit_transform(y_test.astype('float').round().astype('int').reshape(-1, 1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11555 samples, validate on 1284 samples\n",
      "Epoch 1/3\n",
      "11555/11555 [==============================] - 84s 7ms/sample - loss: 1.0461 - accuracy: 0.6168 - val_loss: 0.8824 - val_accuracy: 0.6542\n",
      "Epoch 2/3\n",
      "11555/11555 [==============================] - 83s 7ms/sample - loss: 0.7723 - accuracy: 0.7032 - val_loss: 0.8920 - val_accuracy: 0.6480\n",
      "Epoch 3/3\n",
      "11555/11555 [==============================] - 83s 7ms/sample - loss: 0.5872 - accuracy: 0.7797 - val_loss: 0.9577 - val_accuracy: 0.6410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f760fcdcb70>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Embedding(28557, 128))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.005), loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model.fit(X_train_final, y_train_ohe, batch_size=128, epochs=3, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_final).argmax(axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.51      0.50       521\n",
      "           2       0.22      0.06      0.09       314\n",
      "           3       0.27      0.13      0.17       447\n",
      "           4       0.31      0.47      0.37       924\n",
      "           5       0.81      0.80      0.80      3297\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      5503\n",
      "   macro avg       0.42      0.39      0.39      5503\n",
      "weighted avg       0.62      0.62      0.61      5503\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6180265309831001"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(y_test.round().astype('int'), predictions))\n",
    "accuracy_score(y_test.round().astype('int'), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM (binary target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bin = (y_train.round() > 3).astype('int')\n",
    "y_test_bin = (y_test.round() > 3).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10271 samples, validate on 2568 samples\n",
      "Epoch 1/3\n",
      "10271/10271 [==============================] - 98s 10ms/sample - loss: 0.4115 - accuracy: 0.8258 - val_loss: 0.3358 - val_accuracy: 0.8575\n",
      "Epoch 2/3\n",
      "10271/10271 [==============================] - 100s 10ms/sample - loss: 0.2275 - accuracy: 0.9170 - val_loss: 0.3436 - val_accuracy: 0.8621\n",
      "Epoch 3/3\n",
      "10271/10271 [==============================] - 100s 10ms/sample - loss: 0.1196 - accuracy: 0.9605 - val_loss: 0.4021 - val_accuracy: 0.8567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f75f0c28e80>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Embedding(28557, 128))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01), loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
    "model.fit(X_train_final, y_train_bin, batch_size=64, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.58      0.66      1282\n",
      "           1       0.88      0.95      0.91      4221\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      5503\n",
      "   macro avg       0.82      0.76      0.78      5503\n",
      "weighted avg       0.85      0.86      0.85      5503\n",
      "\n",
      "0.8971338242300124\n",
      "0.8597128838815191\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_bin, predictions > 0.5))\n",
    "print(roc_auc_score(y_test_bin, predictions))\n",
    "print(accuracy_score(y_test_bin, predictions > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext model with multiclass target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def get_row_ngrams(row, ngram_range):\n",
    "    return set(zip(*[row[i:] for i in range(ngram_range)]))\n",
    "\n",
    "\n",
    "def get_all_ngrams(dataset, ngram_range):\n",
    "    ngramed_dataset = map(partial(get_row_ngrams, ngram_range=ngram_range), dataset)\n",
    "    return reduce(lambda x, y: x.union(y), ngramed_dataset)\n",
    "\n",
    "\n",
    "def build_ngram_mapping(train, test, ngram_range):\n",
    "    max_features = max(train.apply(lambda x: max(x)).max(), test.apply(lambda x: max(x)).max())\n",
    "    train_ngrams = get_all_ngrams(train, ngram_range)\n",
    "    test_ngrams = get_all_ngrams(test, ngram_range)\n",
    "    return {ngram: i + max_features + 1 for i, ngram in enumerate(train_ngrams.union(test_ngrams))}\n",
    "\n",
    "\n",
    "def add_ngrams_to_row(row, ngram_mapping, ngram_range):\n",
    "    existing_ngrams = get_row_ngrams(row, ngram_range)\n",
    "    return list(row) + [ngram_mapping[ngram] for ngram in existing_ngrams]\n",
    "\n",
    "\n",
    "def add_ngrams_to_dataset(dataset, ngram_mapping, ngram_range):\n",
    "    return dataset.apply(lambda row: add_ngrams_to_row(row, ngram_mapping, ngram_range))\n",
    "\n",
    "\n",
    "def add_ngrams(train, test, ngram_range=2):\n",
    "    ngram_mapping = build_ngram_mapping(train, test, ngram_range)\n",
    "    return add_ngrams_to_dataset(train, ngram_mapping, ngram_range), add_ngrams_to_dataset(test, ngram_mapping, ngram_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_for_ngrams = pd.Series(list(pad_sequences(X_train_encoded, maxlen=128)))\n",
    "X_test_for_ngrams = pd.Series(list(pad_sequences(X_test_encoded, maxlen=128)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ngramed, X_test_ngramed = add_ngrams(X_train_for_ngrams, X_test_for_ngrams, ngram_range=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ngramed = pad_sequences(X_train_ngramed, maxlen=256)\n",
    "X_test_ngramed = pad_sequences(X_test_ngramed, maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377631"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = max(X_train_ngramed.max(), X_test_ngramed.max()) + 1\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10271 samples, validate on 2568 samples\n",
      "Epoch 1/4\n",
      "10271/10271 [==============================] - 50s 5ms/sample - loss: 1.1826 - accuracy: 0.5849 - val_loss: 1.0222 - val_accuracy: 0.6121\n",
      "Epoch 2/4\n",
      "10271/10271 [==============================] - 50s 5ms/sample - loss: 0.7723 - accuracy: 0.7019 - val_loss: 0.8307 - val_accuracy: 0.6686\n",
      "Epoch 3/4\n",
      "10271/10271 [==============================] - 50s 5ms/sample - loss: 0.3723 - accuracy: 0.8933 - val_loss: 0.8206 - val_accuracy: 0.6807\n",
      "Epoch 4/4\n",
      "10271/10271 [==============================] - 49s 5ms/sample - loss: 0.1625 - accuracy: 0.9709 - val_loss: 0.8445 - val_accuracy: 0.6733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f93721e55c0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=256))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_ngramed, y_train_ohe, batch_size=128, epochs=4, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.50      0.54       527\n",
      "           2       0.28      0.06      0.10       311\n",
      "           3       0.30      0.24      0.27       449\n",
      "           4       0.42      0.39      0.40       904\n",
      "           5       0.80      0.91      0.85      3312\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      5503\n",
      "   macro avg       0.47      0.42      0.43      5503\n",
      "weighted avg       0.64      0.68      0.66      5503\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6821733599854625"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test_ngramed).argmax(axis=1) + 1\n",
    "print(classification_report(y_test.round().astype('int'), predictions))\n",
    "accuracy_score(y_test.round().astype('int'), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ngramed, X_test_ngramed = add_ngrams(X_train_for_ngrams, X_test_for_ngrams, ngram_range=3)\n",
    "X_train_ngramed = pad_sequences(X_train_ngramed, maxlen=512)\n",
    "X_test_ngramed = pad_sequences(X_test_ngramed, maxlen=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "710743"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = max(X_train_ngramed.max(), X_test_ngramed.max()) + 1\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10271 samples, validate on 2568 samples\n",
      "Epoch 1/6\n",
      "10271/10271 [==============================] - 93s 9ms/sample - loss: 1.2310 - accuracy: 0.5863 - val_loss: 1.1565 - val_accuracy: 0.6083\n",
      "Epoch 2/6\n",
      "10271/10271 [==============================] - 94s 9ms/sample - loss: 1.1464 - accuracy: 0.5922 - val_loss: 1.1086 - val_accuracy: 0.6083\n",
      "Epoch 3/6\n",
      "10271/10271 [==============================] - 93s 9ms/sample - loss: 0.9977 - accuracy: 0.6097 - val_loss: 1.0060 - val_accuracy: 0.6180\n",
      "Epoch 4/6\n",
      "10271/10271 [==============================] - 93s 9ms/sample - loss: 0.7613 - accuracy: 0.7071 - val_loss: 0.9455 - val_accuracy: 0.6293\n",
      "Epoch 5/6\n",
      "10271/10271 [==============================] - 92s 9ms/sample - loss: 0.5426 - accuracy: 0.8201 - val_loss: 0.8849 - val_accuracy: 0.6515\n",
      "Epoch 6/6\n",
      "10271/10271 [==============================] - 92s 9ms/sample - loss: 0.3820 - accuracy: 0.9008 - val_loss: 0.8769 - val_accuracy: 0.6550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f93264c2be0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=512))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(lr=0.005), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_ngramed, y_train_ohe, batch_size=128, epochs=6, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.36      0.42       527\n",
      "           2       0.00      0.00      0.00       311\n",
      "           3       0.31      0.08      0.12       449\n",
      "           4       0.44      0.22      0.30       904\n",
      "           5       0.70      0.96      0.81      3312\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      5503\n",
      "   macro avg       0.39      0.32      0.33      5503\n",
      "weighted avg       0.56      0.65      0.58      5503\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6538251862620389"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test_ngramed).argmax(axis=1) + 1\n",
    "print(classification_report(y_test.round().astype('int'), predictions))\n",
    "accuracy_score(y_test.round().astype('int'), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-generated Russian W2V embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gensim\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming words using Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_url = 'https://raw.githubusercontent.com/akutuzov/universal-pos-tags/4653e8a9154e93fe2f417c7fdb7a357b7d6ce333/ru-rnc.map'\n",
    "\n",
    "mystem2upos = {}\n",
    "r = requests.get(mapping_url, stream=True)\n",
    "for pair in r.text.split('\\n'):\n",
    "    pair = pair.split()\n",
    "    if len(pair) > 1:\n",
    "        mystem2upos[pair[0]] = pair[1]\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, mapping):\n",
    "        self.m = Mystem()\n",
    "        self.mapping = mapping\n",
    "        \n",
    "    def process(self, text, postags=True):\n",
    "        processed = self.m.analyze(text)\n",
    "        tagged = []\n",
    "        for w in processed:\n",
    "            try:\n",
    "                lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "                pos = w[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "                pos = pos.split('=')[0].strip()\n",
    "                pos = self.mapping.get(pos, 'X')\n",
    "                tagged.append(lemma.lower() + '_' + pos)\n",
    "            except:\n",
    "                continue\n",
    "        if not postags:\n",
    "            tagged = [t.split('_')[0] for t in tagged]\n",
    "        return tagged\n",
    "\n",
    "processor = Preprocessor(mystem2upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mystem = X_train.apply(lambda string: processor.process(string))\n",
    "X_test_mystem = X_test.apply(lambda string: processor.process(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building dictionary and encoding datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = reduce(lambda x, y: set(x).union(set(y)), chain(X_train_mystem, X_test_mystem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {pair[1]: pair[0] + 1 for pair in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(dataset):\n",
    "    return dataset.apply(lambda words: [dictionary[word] for word in words])\n",
    "\n",
    "X_train_w2v = encode(X_train_mystem)\n",
    "X_test_w2v = encode(X_test_mystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = pad_sequences(X_train_w2v, maxlen=256)\n",
    "X_test_w2v = pad_sequences(X_test_w2v, maxlen=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and building pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0505 12:28:29.907680 139727064356672 smart_open_lib.py:379] this function is deprecated, use smart_open.open instead\n"
     ]
    }
   ],
   "source": [
    "model_path = 'araneum_upos_skipgram_300_2_2018.vec.gz'\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5009\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros(shape=[len(words) + 1, EMBEDDING_DIM])\n",
    "not_found_count = 0\n",
    "\n",
    "for word, idx in dictionary.items():\n",
    "    try:\n",
    "        embedding_matrix[idx] = w2v_model.get_vector(word)\n",
    "    except:\n",
    "        not_found_count += 1\n",
    "        continue\n",
    "print(not_found_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11555 samples, validate on 1284 samples\n",
      "Epoch 1/2\n",
      "11555/11555 [==============================] - 17s 1ms/sample - loss: 1.2293 - accuracy: 0.5865 - val_loss: 1.1457 - val_accuracy: 0.6464\n",
      "Epoch 2/2\n",
      "11555/11555 [==============================] - 17s 1ms/sample - loss: 1.2159 - accuracy: 0.5909 - val_loss: 1.1188 - val_accuracy: 0.6464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1386e2d6d8>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(dictionary) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=256, trainable=False))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_w2v, y_train_ohe, batch_size=128, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       507\n",
      "           1       0.00      0.00      0.00       322\n",
      "           2       0.00      0.00      0.00       444\n",
      "           3       0.00      0.00      0.00       932\n",
      "           4       0.60      1.00      0.75      3298\n",
      "\n",
      "   micro avg       0.60      0.60      0.60      5503\n",
      "   macro avg       0.12      0.20      0.15      5503\n",
      "weighted avg       0.36      0.60      0.45      5503\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5993094675631474"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test_w2v).argmax(axis=1)\n",
    "print(classification_report(y_test_ohe.argmax(axis=1), predictions))\n",
    "accuracy_score(y_test_ohe.argmax(axis=1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
